{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31965901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Projects_to_compete\\Testing_Framework_Here\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:01<00:00, 89.61it/s, Materializing param=pooler.dense.weight]                              \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text embeddings...\n",
      "Embedding dimension: 384\n",
      "\n",
      "============================================================\n",
      "Testing COSINE similarity\n",
      "============================================================\n",
      "\n",
      "Query: \"I like learning about AI\"\n",
      "Top 5 results:\n",
      "1. text_1: similarity=0.6881\n",
      "2. text_3: similarity=0.6638\n",
      "3. text_0: similarity=0.6467\n",
      "4. text_8: similarity=0.4706\n",
      "5. text_6: similarity=0.4290\n",
      "\n",
      "============================================================\n",
      "Testing DOT_PRODUCT similarity\n",
      "============================================================\n",
      "\n",
      "Query: \"I like learning about AI\"\n",
      "Top 5 results:\n",
      "1. text_1: similarity=0.6881\n",
      "2. text_3: similarity=0.6638\n",
      "3. text_0: similarity=0.6467\n",
      "4. text_8: similarity=0.4706\n",
      "5. text_6: similarity=0.4290\n",
      "\n",
      "============================================================\n",
      "Testing EUCLIDEAN similarity\n",
      "============================================================\n",
      "\n",
      "Query: \"I like learning about AI\"\n",
      "Top 5 results:\n",
      "1. text_1: distance=0.7898\n",
      "2. text_3: distance=0.8200\n",
      "3. text_0: distance=0.8406\n",
      "4. text_8: distance=1.0290\n",
      "5. text_6: distance=1.0686\n",
      "\n",
      "============================================================\n",
      "Comparison complete!\n",
      "============================================================\n",
      "\n",
      "Note:\n",
      "- Cosine similarity: Range [-1, 1], higher = more similar\n",
      "- Dot product: Higher = more similar (depends on norms)\n",
      "- Euclidean distance: Lower = more similar\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example demonstrating different similarity metrics\n",
    "using Hugging Face embeddings.\n",
    "\"\"\"\n",
    "\n",
    "from fastvecdb import FastVecDB, SimilarityMetric\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Load embedding model once\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def embed_text(text: str) -> list:\n",
    "    \"\"\"Generate a normalized embedding vector using Hugging Face.\"\"\"\n",
    "    embedding = model.encode(text, normalize_embeddings=True)\n",
    "    return embedding.tolist()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Sample texts\n",
    "    texts = [\n",
    "        \"I love machine learning\",\n",
    "        \"Artificial intelligence is fascinating\",\n",
    "        \"Fast vector databases are useful\",\n",
    "        \"I enjoy reading about deep learning\",\n",
    "        \"Python is my favorite programming language\",\n",
    "        \"Databases store information efficiently\",\n",
    "        \"Neural networks learn patterns\",\n",
    "        \"I like pizza and burgers\",\n",
    "        \"Natural language processing is fun\",\n",
    "        \"I write code every day\",\n",
    "    ]\n",
    "\n",
    "    # Create vectors\n",
    "    print(\"Creating text embeddings...\")\n",
    "    vectors = [(embed_text(text), f\"text_{i}\") for i, text in enumerate(texts)]\n",
    "\n",
    "    dimension = len(vectors[0][0])\n",
    "    print(f\"Embedding dimension: {dimension}\")\n",
    "\n",
    "    # Test each similarity metric\n",
    "    metrics = [\n",
    "        SimilarityMetric.COSINE,\n",
    "        SimilarityMetric.DOT_PRODUCT,\n",
    "        SimilarityMetric.EUCLIDEAN,\n",
    "    ]\n",
    "\n",
    "    for metric in metrics:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing {metric.value.upper()} similarity\")\n",
    "        print('='*60)\n",
    "\n",
    "        # Create database\n",
    "        db = FastVecDB(\n",
    "            storage_path=f\"./example_data_{metric.value}\",\n",
    "            dimension=dimension,\n",
    "            similarity_metric=metric,\n",
    "        )\n",
    "\n",
    "        # Insert vectors\n",
    "        for vector, vec_id in vectors:\n",
    "            db.insert(vec_id, vector, metadata={\"metric\": metric.value})\n",
    "\n",
    "        # Query\n",
    "        query_text = \"I like learning about AI\"\n",
    "        query_vector = embed_text(query_text)\n",
    "\n",
    "        results = db.search(query_vector, top_k=5)\n",
    "\n",
    "        print(f\"\\nQuery: \\\"{query_text}\\\"\")\n",
    "        print(\"Top 5 results:\")\n",
    "\n",
    "        for i, result in enumerate(results, 1):\n",
    "            score = result[\"score\"]\n",
    "            if metric == SimilarityMetric.EUCLIDEAN:\n",
    "                distance = -score\n",
    "                print(f\"{i}. {result['id']}: distance={distance:.4f}\")\n",
    "            else:\n",
    "                print(f\"{i}. {result['id']}: similarity={score:.4f}\")\n",
    "\n",
    "        db.close()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Comparison complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nNote:\")\n",
    "    print(\"- Cosine similarity: Range [-1, 1], higher = more similar\")\n",
    "    print(\"- Dot product: Higher = more similar (depends on norms)\")\n",
    "    print(\"- Euclidean distance: Lower = more similar\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91df8c3",
   "metadata": {},
   "source": [
    "## Cache Use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae4102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 210.20it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FastVecDB Cache Quick Demo (Hugging Face)\n",
      "============================================================\n",
      "\n",
      "1. Creating embeddings...\n",
      "   âœ“ Embedded doc1\n",
      "   âœ“ Embedded doc2\n",
      "   âœ“ Embedded doc3\n",
      "   Embedding dimension: 384\n",
      "\n",
      "2. Creating database with intelligent caching...\n",
      "\n",
      "3. Inserting documents...\n",
      "   âœ“ Inserted doc1\n",
      "   âœ“ Inserted doc2\n",
      "   âœ“ Inserted doc3\n",
      "\n",
      "4. First search (COLD - no cache):\n",
      "   Time: 2.97ms\n",
      "   â†’ doc3 | score=0.6246\n",
      "   â†’ doc2 | score=0.5251\n",
      "\n",
      "5. Second search (WARM - cache hit!):\n",
      "   Time: 0.00ms\n",
      "   â†’ doc3 | score=0.6246\n",
      "   â†’ doc2 | score=0.5251\n",
      "\n",
      "6. Cache Statistics:\n",
      "   Query cache size: 1\n",
      "   Hot vector cache size: 3\n",
      "\n",
      "============================================================\n",
      "Key Benefit: Semantic search + caching = instant results\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FastVecDB Cache Quick Demo - Hugging Face Embeddings\n",
    "\n",
    "Demonstrates how FastVecDB caching speeds up repeated\n",
    "semantic searches using real text embeddings.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from fastvecdb import FastVecDB, SimilarityMetric\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Load embedding model once (important for performance)\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def embed_text(text: str) -> list:\n",
    "    \"\"\"Generate a normalized embedding vector from text.\"\"\"\n",
    "    embedding = model.encode(text, normalize_embeddings=True)\n",
    "    return embedding.tolist()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FastVecDB Cache Quick Demo (Hugging Face)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Sample documents\n",
    "    documents = [\n",
    "        (\"FastVecDB is a fast vector database\", \"doc1\"),\n",
    "        (\"Vector databases enable semantic search\", \"doc2\"),\n",
    "        (\"Caching makes repeated queries faster\", \"doc3\"),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n1. Creating embeddings...\")\n",
    "    vectors = []\n",
    "    for text, doc_id in documents:\n",
    "        vectors.append((embed_text(text), doc_id, {\"text\": text}))\n",
    "        print(f\"   âœ“ Embedded {doc_id}\")\n",
    "\n",
    "    dimension = len(vectors[0][0])\n",
    "    print(f\"   Embedding dimension: {dimension}\")\n",
    "\n",
    "    # Create database with caching enabled\n",
    "    print(\"\\n2. Creating database with intelligent caching...\")\n",
    "    db = FastVecDB(\n",
    "        storage_path=\"./quick_cache_demo_hf\",\n",
    "        dimension=dimension,\n",
    "        similarity_metric=SimilarityMetric.COSINE,\n",
    "        enable_cache=True,\n",
    "    )\n",
    "\n",
    "    # Insert vectors\n",
    "    print(\"\\n3. Inserting documents...\")\n",
    "    for vector, vec_id, metadata in vectors:\n",
    "        db.insert(vec_id, vector, metadata)\n",
    "        print(f\"   âœ“ Inserted {vec_id}\")\n",
    "\n",
    "    # Query text\n",
    "    query_text = \"How does caching improve vector search?\"\n",
    "    query_vector = embed_text(query_text)\n",
    "\n",
    "    # First search (COLD)\n",
    "    print(\"\\n4. First search (COLD - no cache):\")\n",
    "    start = time.time()\n",
    "    results1 = db.search(query_vector, top_k=2)\n",
    "    cold_time = time.time() - start\n",
    "    print(f\"   Time: {cold_time*1000:.2f}ms\")\n",
    "\n",
    "    for r in results1:\n",
    "        print(f\"   â†’ {r['id']} | score={r['score']:.4f}\")\n",
    "\n",
    "    # Second search (WARM)\n",
    "    print(\"\\n5. Second search (WARM - cache hit!):\")\n",
    "    start = time.time()\n",
    "    results2 = db.search(query_vector, top_k=2)  # Same query\n",
    "    warm_time = time.time() - start\n",
    "    print(f\"   Time: {warm_time*1000:.2f}ms\")\n",
    "\n",
    "    for r in results2:\n",
    "        print(f\"   â†’ {r['id']} | score={r['score']:.4f}\")\n",
    "\n",
    "    # Speedup\n",
    "    if warm_time > 0:\n",
    "        speedup = cold_time / warm_time\n",
    "        print(f\"\\n   ðŸš€ Cache made it {speedup:.1f}x faster!\")\n",
    "\n",
    "    # Cache stats\n",
    "    print(\"\\n6. Cache Statistics:\")\n",
    "    stats = db.get_stats()\n",
    "    cache_stats = stats.get(\"cache_stats\", {})\n",
    "    print(f\"   Query cache size: {cache_stats.get('query_cache', {}).get('size', 0)}\")\n",
    "    print(f\"   Hot vector cache size: {cache_stats.get('hot_vector_cache', {}).get('size', 0)}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Key Benefit: Semantic search + caching = instant results\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    db.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b2fed",
   "metadata": {},
   "source": [
    "# RAG Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad66453",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example RAG (Retrieval-Augmented Generation) pipeline\n",
    "using FastVecDB + Hugging Face embeddings.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from fastvecdb import FastVecDB, SimilarityMetric\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Load embedding model once\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def embed_text(text: str) -> list:\n",
    "    \"\"\"Generate a normalized embedding for text.\"\"\"\n",
    "    embedding = model.encode(text, normalize_embeddings=True)\n",
    "    return embedding.tolist()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Knowledge base documents\n",
    "    documents = [\n",
    "        {\n",
    "            \"id\": \"doc1\",\n",
    "            \"text\": \"Python is a high-level programming language known for its simplicity and readability.\",\n",
    "            \"category\": \"programming\",\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc2\",\n",
    "            \"text\": \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "            \"category\": \"ai\",\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc3\",\n",
    "            \"text\": \"Vector databases are specialized databases designed to store and query high-dimensional vectors.\",\n",
    "            \"category\": \"databases\",\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc4\",\n",
    "            \"text\": \"FastVecDB is a pure-Python vector search framework with intelligent caching.\",\n",
    "            \"category\": \"databases\",\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc5\",\n",
    "            \"text\": \"RAG combines retrieval of relevant documents with language model generation.\",\n",
    "            \"category\": \"ai\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    print(\"Initializing FastVecDB for RAG pipeline...\")\n",
    "\n",
    "    # Create embeddings first to get dimension\n",
    "    print(\"\\nCreating document embeddings...\")\n",
    "    vectors = []\n",
    "    for doc in documents:\n",
    "        vector = embed_text(doc[\"text\"])\n",
    "        vectors.append((doc, vector))\n",
    "        print(f\"  Embedded: {doc['id']}\")\n",
    "\n",
    "    dimension = len(vectors[0][1])\n",
    "    print(f\"Embedding dimension: {dimension}\")\n",
    "\n",
    "    db = FastVecDB(\n",
    "        storage_path=\"./rag_example_data_hf\",\n",
    "        dimension=dimension,\n",
    "        similarity_metric=SimilarityMetric.COSINE,\n",
    "        enable_cache=True,\n",
    "    )\n",
    "\n",
    "    # Index documents\n",
    "    print(\"\\nIndexing documents...\")\n",
    "    for doc, vector in vectors:\n",
    "        db.insert(\n",
    "            vector_id=doc[\"id\"],\n",
    "            vector=vector,\n",
    "            metadata={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"category\": doc[\"category\"],\n",
    "            },\n",
    "            bucket_id=\"knowledge_base\",\n",
    "        )\n",
    "        print(f\"  Indexed: {doc['id']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RAG Query Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    queries = [\n",
    "        \"What is Python?\",\n",
    "        \"Tell me about vector databases\",\n",
    "        \"How does machine learning work?\",\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        query_vector = embed_text(query)\n",
    "\n",
    "        # Retrieve documents\n",
    "        results = db.search(\n",
    "            query_vector=query_vector,\n",
    "            top_k=3,\n",
    "            bucket_ids=[\"knowledge_base\"],\n",
    "        )\n",
    "\n",
    "        print(f\"Retrieved {len(results)} relevant documents:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {result['id']} (score: {result['score']:.4f})\")\n",
    "            print(f\"   Category: {result['metadata']['category']}\")\n",
    "            print(f\"   Text: {result['metadata']['text']}\")\n",
    "\n",
    "        # In a real RAG pipeline:\n",
    "        # - Combine retrieved texts into context\n",
    "        # - Send (context + query) to an LLM\n",
    "        # - Return generated answer\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Cache Performance Test\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    query_vector = embed_text(\"Python programming\")\n",
    "\n",
    "    # Cold query\n",
    "    start = time.time()\n",
    "    _ = db.search(query_vector, top_k=5)\n",
    "    time1 = time.time() - start\n",
    "\n",
    "    # Warm query\n",
    "    start = time.time()\n",
    "    _ = db.search(query_vector, top_k=5)\n",
    "    time2 = time.time() - start\n",
    "\n",
    "    print(f\"First query (cold): {time1 * 1000:.2f}ms\")\n",
    "    print(f\"Second query (warm): {time2 * 1000:.2f}ms\")\n",
    "    if time2 > 0:\n",
    "        print(f\"Speedup: {time1 / time2:.1f}x\")\n",
    "\n",
    "    stats = db.get_stats()\n",
    "    print(\"\\nDatabase stats:\")\n",
    "    print(f\"  Total documents: {stats['total_vectors']}\")\n",
    "    if stats.get(\"cache_stats\"):\n",
    "        print(f\"  Cache stats: {stats['cache_stats']}\")\n",
    "\n",
    "    db.close()\n",
    "    print(\"\\nRAG pipeline example complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfe96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 166.73it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:01<00:00, 171.03it/s, Materializing param=shared.weight]                                                       \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AfmoeForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'BltForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'CwmForCausalLM', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'ExaoneMoeForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FlexOlmoForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'Glm4MoeLiteForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'Jais2ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxM2ForCausalLM', 'MinistralForCausalLM', 'Ministral3ForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NanoChatForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'Qwen3NextForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'SolarOpenForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TrOCRForCausalLM', 'VaultGemmaForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'YoutuForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "User Query: What is Python?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ''}]\n",
      "\n",
      "Retrieved Documents:\n",
      "- doc1 | programming\n",
      "- doc2 | ai\n",
      "- doc5 | ai\n",
      "\n",
      "LLM Answer:\n",
      "\n",
      "============================================================\n",
      "User Query: Explain vector databases\n",
      "============================================================\n",
      "[{'generated_text': ''}]\n",
      "\n",
      "Retrieved Documents:\n",
      "- doc2 | ai\n",
      "- doc1 | programming\n",
      "- doc5 | ai\n",
      "\n",
      "LLM Answer:\n",
      "\n",
      "============================================================\n",
      "User Query: How does machine learning work?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ''}]\n",
      "\n",
      "Retrieved Documents:\n",
      "- doc2 | ai\n",
      "- doc5 | ai\n",
      "- doc1 | programming\n",
      "\n",
      "LLM Answer:\n",
      "\n",
      "============================================================\n",
      "Cache Performance Test\n",
      "============================================================\n",
      "Cold query: 2.71 ms\n",
      "Warm query: 0.79 ms\n",
      "Speedup: 3.4x\n",
      "\n",
      "RAG pipeline complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fully functional RAG (Retrieval-Augmented Generation) pipeline\n",
    "using FastVecDB + Hugging Face embeddings + Hugging Face LLM (FLAN-T5-small).\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from fastvecdb import FastVecDB, SimilarityMetric\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# 1. LOAD MODELS\n",
    "# -----------------------------\n",
    "\n",
    "# Load Hugging Face embedding model (SentenceTransformers)\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load Hugging Face instruction-tuned LLM (FLAN-T5-small)\n",
    "llm = pipeline(\n",
    "    task=\"text-generation\",      # Must use text-generation\n",
    "    model=\"google/flan-t5-small\",\n",
    "    max_new_tokens=200\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. EMBEDDING FUNCTION\n",
    "# -----------------------------\n",
    "\n",
    "def embed_text(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Convert text into a normalized vector embedding compatible with FastVecDB.\n",
    "    \"\"\"\n",
    "    embedding = embedding_model.encode(text, normalize_embeddings=True)\n",
    "    return embedding.tolist()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. MAIN RAG PIPELINE\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # -----------------------------\n",
    "    # KNOWLEDGE BASE DOCUMENTS\n",
    "    # -----------------------------\n",
    "    documents = [\n",
    "        {\"id\": \"doc1\", \"text\": \"Python is a high-level programming language known for its simplicity and readability.\", \"category\": \"programming\"},\n",
    "        {\"id\": \"doc2\", \"text\": \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\", \"category\": \"ai\"},\n",
    "        {\"id\": \"doc3\", \"text\": \"Vector databases are specialized databases designed to store and query high-dimensional vectors.\", \"category\": \"databases\"},\n",
    "        {\"id\": \"doc4\", \"text\": \"FastVecDB is a pure-Python vector search framework with intelligent caching.\", \"category\": \"databases\"},\n",
    "        {\"id\": \"doc5\", \"text\": \"RAG combines retrieval of relevant documents with language model generation.\", \"category\": \"ai\"},\n",
    "    ]\n",
    "\n",
    "    print(\"Initializing RAG pipeline...\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # GENERATE DOCUMENT EMBEDDINGS\n",
    "    # -----------------------------\n",
    "    vectors = []\n",
    "    for doc in documents:\n",
    "        vector = embed_text(doc[\"text\"])\n",
    "        vectors.append((doc, vector))\n",
    "\n",
    "    dimension = len(vectors[0][1])\n",
    "\n",
    "    # -----------------------------\n",
    "    # INITIALIZE FASTVECDB\n",
    "    # -----------------------------\n",
    "    db = FastVecDB(\n",
    "        storage_path=\"./rag_example_data_hf\",\n",
    "        dimension=dimension,\n",
    "        similarity_metric=SimilarityMetric.COSINE,\n",
    "        enable_cache=True\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # INDEX DOCUMENTS\n",
    "    # -----------------------------\n",
    "    for doc, vector in vectors:\n",
    "        db.insert(\n",
    "            vector_id=doc[\"id\"],\n",
    "            vector=vector,\n",
    "            metadata={\"text\": doc[\"text\"], \"category\": doc[\"category\"]},\n",
    "            bucket_id=\"knowledge_base\"\n",
    "        )\n",
    "\n",
    "    # -----------------------------\n",
    "    # RAG QUERY LOOP\n",
    "    # -----------------------------\n",
    "    queries = [\n",
    "        \"What is Python?\",\n",
    "        \"Explain vector databases\",\n",
    "        \"How does machine learning work?\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"User Query: {query}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # -----------------------------\n",
    "        # QUERY EMBEDDING\n",
    "        # -----------------------------\n",
    "        query_vector = embed_text(query)\n",
    "\n",
    "        # -----------------------------\n",
    "        # VECTOR RETRIEVAL\n",
    "        # -----------------------------\n",
    "        results = db.search(query_vector, top_k=5, bucket_ids=[\"knowledge_base\"])\n",
    "\n",
    "        # -----------------------------\n",
    "        # METADATA FILTERING (POST-SEARCH)\n",
    "        # -----------------------------\n",
    "        filtered_results = [\n",
    "            r for r in results\n",
    "            if r[\"metadata\"][\"category\"] in {\"ai\", \"programming\"}\n",
    "        ]\n",
    "\n",
    "        # -----------------------------\n",
    "        # CONTEXT CONSTRUCTION\n",
    "        # -----------------------------\n",
    "        context = \"\\n\".join(f\"- {r['metadata']['text']}\" for r in filtered_results)\n",
    "\n",
    "        # -----------------------------\n",
    "        # PROMPT CONSTRUCTION\n",
    "        # -----------------------------\n",
    "        prompt = f\"\"\"\n",
    "Answer the question using only the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # LLM GENERATION (FLAN-T5)\n",
    "        # -----------------------------\n",
    "        # response = llm(\n",
    "        #     prompt,\n",
    "        #     max_new_tokens=200,\n",
    "        #     do_sample=True,\n",
    "        #     return_full_text=False\n",
    "        # )[0][\"generated_text\"]\n",
    "        response = llm(\n",
    "            prompt,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            return_full_text=False\n",
    "        )\n",
    "        # print(response)\n",
    "\n",
    "        # -----------------------------\n",
    "        # OUTPUT\n",
    "        # -----------------------------\n",
    "        print(\"\\nRetrieved Documents:\")\n",
    "        for r in filtered_results:\n",
    "            print(f\"- {r['id']} | {r['metadata']['category']}\")\n",
    "\n",
    "        print(\"\\nLLM Answer:\")\n",
    "        print(response)\n",
    "\n",
    "    # -----------------------------\n",
    "    # CACHE PERFORMANCE DEMO\n",
    "    # -----------------------------\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Cache Performance Test\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    query_vector = embed_text(\"Python programming\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    db.search(query_vector, top_k=3)\n",
    "    cold_time = time.perf_counter() - start\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    db.search(query_vector, top_k=3)\n",
    "    warm_time = time.perf_counter() - start\n",
    "\n",
    "    print(f\"Cold query: {cold_time*1000:.2f} ms\")\n",
    "    print(f\"Warm query: {warm_time*1000:.2f} ms\")\n",
    "    if warm_time > 0:\n",
    "        print(f\"Speedup: {cold_time / warm_time:.1f}x\")\n",
    "    else:\n",
    "        print(\"Speedup: warm query was instantaneous (too fast to measure) âœ…\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # CLEANUP\n",
    "    # -----------------------------\n",
    "    db.close()\n",
    "    print(\"\\nRAG pipeline complete!\")\n",
    "\n",
    "# -----------------------------\n",
    "# RUN\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72efd2d9",
   "metadata": {},
   "source": [
    "## Cache Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3761993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FastVecDB Cache Demo with Hugging Face Embeddings\n",
      "============================================================\n",
      "\n",
      "1. Loading Hugging Face embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 135.35it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Creating database with intelligent caching...\n",
      "\n",
      "3. Inserting embeddings into FastVecDB...\n",
      "   âœ“ Inserted doc1\n",
      "   âœ“ Inserted doc2\n",
      "   âœ“ Inserted doc3\n",
      "   âœ“ Inserted doc4\n",
      "\n",
      "4. First search (COLD - no cache):\n",
      "   Time: 65.07ms\n",
      "   Found: doc4 (score: 0.7179)\n",
      "   Found: doc1 (score: 0.5489)\n",
      "\n",
      "5. Second search (WARM - cache hit!):\n",
      "   Time: 0.00ms\n",
      "   Found: doc4 (score: 0.7179)\n",
      "   Found: doc1 (score: 0.5489)\n",
      "\n",
      "6. Cache Statistics:\n",
      "   Query cache: 1 queries cached\n",
      "   Hot vectors: 4 vectors cached\n",
      "\n",
      "============================================================\n",
      "Key Benefit: Repeated queries with real embeddings are instant!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FastVecDB Cache Quick Demo with Hugging Face Embeddings\n",
    "\n",
    "This demo illustrates how enabling caching in FastVecDB\n",
    "can dramatically speed up repeated similarity searches using\n",
    "real embeddings from a Hugging Face SentenceTransformer model.\n",
    "\"\"\"\n",
    "\n",
    "import time  # Used to measure execution time\n",
    "from fastvecdb import FastVecDB, SimilarityMetric  # Vector database and similarity metric\n",
    "from sentence_transformers import SentenceTransformer  # Pre-trained NLP embeddings\n",
    "\n",
    "def main():\n",
    "    # ------------------------------------------------------\n",
    "    # 0. Header\n",
    "    # ------------------------------------------------------\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FastVecDB Cache Demo with Hugging Face Embeddings\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 1. Load the Hugging Face embedding model\n",
    "    # ------------------------------------------------------\n",
    "    print(\"\\n1. Loading Hugging Face embedding model...\")\n",
    "    # all-MiniLM-L6-v2 is a lightweight model with 384-dimensional embeddings\n",
    "    # Small enough to be fast, but still produces meaningful semantic vectors\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")  \n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 2. Prepare example documents\n",
    "    # ------------------------------------------------------\n",
    "    # Each document has a unique ID and some text content\n",
    "    documents = [\n",
    "        {\"id\": \"doc1\", \"text\": \"FastVecDB is a high-performance vector database.\"},\n",
    "        {\"id\": \"doc2\", \"text\": \"Caching can make repeated queries much faster.\"},\n",
    "        {\"id\": \"doc3\", \"text\": \"Hugging Face provides state-of-the-art NLP models.\"},\n",
    "        {\"id\": \"doc4\", \"text\": \"Vector search allows finding similar items quickly.\"},\n",
    "    ]\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 3. Initialize FastVecDB with caching enabled\n",
    "    # ------------------------------------------------------\n",
    "    print(\"\\n2. Creating database with intelligent caching...\")\n",
    "    db = FastVecDB(\n",
    "        storage_path=\"./huggingface_cache_demo\",   # Where the database is stored on disk\n",
    "        dimension=384,                              # Must match the embedding size from the model\n",
    "        similarity_metric=SimilarityMetric.COSINE, # Use cosine similarity to compare vectors\n",
    "        enable_cache=True                           # Enable caching for faster repeated queries\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 4. Insert document embeddings into the database\n",
    "    # ------------------------------------------------------\n",
    "    print(\"\\n3. Inserting embeddings into FastVecDB...\")\n",
    "    for doc in documents:\n",
    "        # Convert the text into a numerical vector (embedding)\n",
    "        embedding = model.encode(doc[\"text\"]).tolist()\n",
    "        # Insert the embedding and metadata into the database\n",
    "        db.insert(doc[\"id\"], embedding, metadata={\"text\": doc[\"text\"]})\n",
    "        print(f\"   âœ“ Inserted {doc['id']}\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 5. Prepare a query embedding\n",
    "    # ------------------------------------------------------\n",
    "    # The query text we want to find similar documents for\n",
    "    query_text = \"How can I search vectors quickly?\"\n",
    "    # Encode the query into an embedding vector\n",
    "    query_embedding = model.encode(query_text).tolist()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 6. First search (COLD cache)\n",
    "    # ------------------------------------------------------\n",
    "    # Since this is the first time searching, the cache is empty\n",
    "    print(\"\\n4. First search (COLD - no cache):\")\n",
    "    start = time.time()\n",
    "    results1 = db.search(query_embedding, top_k=2)  # Retrieve top 2 most similar documents\n",
    "    cold_time = time.time() - start  # Measure time taken\n",
    "    print(f\"   Time: {cold_time*1000:.2f}ms\")\n",
    "    # Display results\n",
    "    for res in results1:\n",
    "        print(f\"   Found: {res['id']} (score: {res['score']:.4f})\")\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # 7. Second search (WARM cache)\n",
    "    # ------------------------------------------------------\n",
    "    # Now the cache has stored information from the first search\n",
    "    # This makes retrieval much faster\n",
    "    print(\"\\n5. Second search (WARM - cache hit!):\")\n",
    "    start = time.time()\n",
    "    results2 = db.search(query_embedding, top_k=2)\n",
    "    warm_time = time.time() - start\n",
    "    print(f\"   Time: {warm_time*1000:.2f}ms\")\n",
    "    for res in results2:\n",
    "        print(f\"   Found: {res['id']} (score: {res['score']:.4f})\")\n",
    "\n",
    "    # Calculate and display speedup due to caching\n",
    "    if warm_time > 0:\n",
    "        speedup = cold_time / warm_time\n",
    "        print(f\"\\n   ðŸš€ Cache made it {speedup:.1f}x faster!\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 8. Show cache statistics\n",
    "    # ------------------------------------------------------\n",
    "    print(\"\\n6. Cache Statistics:\")\n",
    "    stats = db.get_stats()\n",
    "    if stats.get(\"cache_stats\"):\n",
    "        cache_stats = stats[\"cache_stats\"]\n",
    "        # Number of query results stored in the cache\n",
    "        print(f\"   Query cache: {cache_stats.get('query_cache', {}).get('size', 0)} queries cached\")\n",
    "        # Number of vectors kept in the hot vector cache\n",
    "        print(f\"   Hot vectors: {cache_stats.get('hot_vector_cache', {}).get('size', 0)} vectors cached\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 9. Footer\n",
    "    # ------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Key Benefit: Repeated queries with real embeddings are instant!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Close database to release resources properly\n",
    "    db.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
